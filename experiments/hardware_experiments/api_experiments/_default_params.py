NUM_ENV_STEPS_BETWEEN_UPDATES = 16
NUM_ENVS = 64
sac_num_env_steps = 1_000_000
horizon_len = 50

SAC_KWARGS = dict(num_timesteps=sac_num_env_steps,
                  num_evals=20,
                  reward_scaling=10,
                  episode_length=horizon_len,
                  episode_length_eval=2 * horizon_len,
                  action_repeat=1,
                  discounting=0.99,
                  lr_policy=3e-4,
                  lr_alpha=3e-4,
                  lr_q=3e-4,
                  num_envs=NUM_ENVS,
                  batch_size=64,
                  grad_updates_per_step=NUM_ENV_STEPS_BETWEEN_UPDATES * NUM_ENVS,
                  num_env_steps_between_updates=NUM_ENV_STEPS_BETWEEN_UPDATES,
                  tau=0.005,
                  wd_policy=0,
                  wd_q=0,
                  wd_alpha=0,
                  num_eval_envs=2 * NUM_ENVS,
                  max_replay_size=5 * 10 ** 4,
                  min_replay_size=2 ** 11,
                  policy_hidden_layer_sizes=(64, 64),
                  critic_hidden_layer_sizes=(64, 64),
                  normalize_observations=True,
                  deterministic_eval=True,
                  wandb_logging=True)
